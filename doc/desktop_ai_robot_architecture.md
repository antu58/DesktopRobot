# 桌面 AI 机器人技术方案（Linux 主控 + 行为控制 + MCU）

> 本文档基于前序讨论整理，目标是构建一个**小体积、可扩展、工程化可落地**的桌面 AI 机器人系统。

---

## 1. 设计目标概述

机器人具备以下能力：
- 语音交互（麦克风 + 扬声器）
- 视觉感知（摄像头）
- 云台（俯仰 / 水平旋转）
- 表情显示屏（状态 / 情绪表达）
- 物理按钮
- Linux 主控（可运行本地模型）
- 可扩展控制电脑与智能家居

**核心原则：高层智能在 Linux，实时执行在 MCU，统一由行为控制层调度。**

---

## 2. 总体分层架构

```text
┌────────────────────────────┐
│        用户交互             │
│  语音 / 视觉 / 按钮         │
└────────────┬───────────────┘
             ▼
┌────────────────────────────┐
│ 感知层（Linux）             │
│ - ASR 语音识别              │
│ - 视觉识别 / 跟踪           │
└────────────┬───────────────┘
             ▼
┌────────────────────────────┐
│ LLM / 意图理解层            │
│ - 自然语言 → 结构化意图    │
│ - 不直接控制任何硬件        │
└────────────┬───────────────┘
             ▼
┌────────────────────────────┐
│ 行为控制层（Go，核心）     │
│ - 状态机 / 行为仲裁        │
│ - 多设备协调               │
└──────┬──────┬──────┬──────┘
       │      │      │
       ▼      ▼      ▼
┌────────┐ ┌────────┐ ┌────────┐
│ 屏幕UI │ │ 音频服务│ │ MCU桥接 │
│ Linux  │ │ Linux  │ │ 串口/USB│
└────────┘ └────────┘ └────────┘
```

---

## 3. 各层职责边界（非常重要）

### 3.1 LLM 层
- 只负责 **语义理解**
- 输出结构化 JSON
- **绝不直接操作设备**

示例：
```json
{
  "intent": "look_up",
  "angle": 30
}
```

---

### 3.2 行为控制层（Go）

这是整个系统的 **“大脑中枢 / 总导演”**。

职责：
- 行为状态管理（Idle / Listening / Speaking / Tracking）
- 行为冲突仲裁（避免边说话边突然大幅转头）
- 参数映射（语义 → 物理量）
- 调度各子系统并行执行

示例：
```text
intent: look_up
↓
pitch = +30°
↓
并行触发：
- 屏幕：attentive
- 音频：ack
- MCU：云台上仰
```

---

## 4. 屏幕显示系统设计（不需要 MCU）

### 4.1 为什么屏幕不该用 MCU
- 需要字体、动画、表情、状态切换
- MCU 不适合复杂 UI
- Linux 图形与 UI 生态成熟

**结论：屏幕直接接 Linux。**

---

### 4.2 屏幕进程设计

- 独立常驻进程（如 `screen-ui`）
- 技术栈可选：
  - Flutter
  - Qt
  - SDL
  - WebView
- 通过 IPC 接收指令

示例指令：
```json
{
  "type": "expression",
  "name": "happy",
  "duration": 3000
}
```

行为层负责：
```go
screen.Show("happy")
```

---

### 4.3 表情 = 状态映射

| 行为状态 | 表情 |
|--------|------|
| Idle | 呼吸动画 |
| Listening | 😐 |
| Thinking | 🤔 |
| Speaking | 😄 |
| Error | 😵 |

行为层只关心“状态”，UI 决定“怎么画”。

---

## 5. 扬声器 / 音频系统设计（不需要 MCU）

### 5.1 推荐方案
- Linux 音频栈：ALSA / PulseAudio / PipeWire
- 独立音频服务进程

示例指令：
```json
{
  "action": "play",
  "file": "ack.wav",
  "interrupt": true
}
```

行为层调用：
```go
audio.Play("ack.wav", Interrupt)
```

---

### 5.2 音频资源管理（关键）
- 是否允许打断
- 是否排队
- 当前是否 Speaking

👉 **音频是共享资源，必须由行为层统一管理。**

---

## 6. MCU 的明确职责边界

| 模块 | 是否 MCU | 原因 |
|----|----|----|
| 舵机 / 云台 | ✅ | PWM、实时性 |
| LED 灯效 | ✅ | 精确定时 |
| 按钮 | ⚠️ | 可 MCU 或 GPIO |
| 屏幕 | ❌ | UI 复杂 |
| 扬声器 | ❌ | Linux 更合适 |

---

## 7. Linux ↔ MCU 通信模型

- 通信方式：USB CDC（虚拟串口）
- Linux 发送高层动作
- MCU 负责：
  - 限位
  - 平滑插值
  - 舵机保护

示例协议：
```json
{
  "cmd": "gimbal_pitch",
  "value": 30
}
```

---

## 8. 行为示例：「抬头看看我」

```text
用户语音
 → ASR
 → LLM（intent=look_up）
 → 行为层：
    - state = Tracking
    - screen = attentive
    - audio = “好的”
    - mcu = pitch +30°
```

**一个意图 = 多设备协同行为。**

---

## 9. 推荐项目结构

```text
robot/
├── behavior/        # Go 行为控制核心
├── screen-ui/       # 屏幕 UI
├── audio-service/   # 音频播放 / TTS
├── mcu-bridge/      # 串口通信
├── perception/      # ASR / Vision
└── llm-adapter/     # LLM 封装
```

---

## 10. 与 StackChan 的定位关系

| 项目 | StackChan | 本方案 |
|----|----|----|
| 主控 | ESP32 | Linux |
| LLM | 云端 | 本地 / 云 |
| 扩展性 | 玩具级 | 产品级 |
| 架构清晰度 | 低 | 高 |

**StackChan 可用于外观与交互灵感，但不适合作为主架构。**

---

## 11. 实施建议（路线）

1. 云台 + MCU + Go 串口控制
2. 行为状态机
3. 屏幕表情系统
4. 音频服务
5. ASR → LLM → 行为闭环

---

