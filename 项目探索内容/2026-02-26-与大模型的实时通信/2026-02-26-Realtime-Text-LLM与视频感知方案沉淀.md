# Realtime Text LLM 与视频感知方案沉淀（2026-02-26）

## 1. 背景与结论

围绕机器人实时对话能力，本轮讨论形成统一结论：

1. 主链路采用 `本地流式 ASR -> Realtime Text LLM -> 本地流式 TTS`，而不是纯云端端到端语音模型。
2. 该路线在成本、可控性、可扩展性（记忆、工具调用、审计）上更优。
3. “Realtime Text LLM”本质是**双向实时会话协议 + 会话状态机**，不是普通 `stream=true` 的单向输出。
4. 在此基础上可增配视频感知支路，通过关键帧抽取与事件摘要注入对话上下文。

## 2. Realtime Text LLM 的准确定义

### 2.1 与常规流式接口的区别

1. 常规 `HTTP + SSE stream=true`：请求发完后，模型单向流式返回文本。
2. Realtime Text LLM（目标形态）：客户端与模型通过长连接（通常 WebSocket）进行双向事件通信，支持会话内增量输入、增量输出、中断与工具回写。

### 2.2 我们需要的核心能力

1. 会话级状态管理（session/conversation）。
2. 双向事件协议（输入文本事件、输出增量事件）。
3. 中断控制（`cancel` 当前回复，立即进入新回合）。
4. 工具调用闭环（函数参数生成、工具执行、结果回写、继续生成）。

## 3. 目标架构（语音转文本主链路）

```text
Mic
  -> VAD + Streaming ASR (partial/final)
  -> Turn Detector (回合判定)
  -> Realtime Text LLM (WS session)
       -> Memory/RAG side channel
       -> Tool Gateway (Function Calling)
  -> Streaming TTS
  -> Speaker
```

设计原则：

1. `partial` 只用于显示和打断判定，不直接触发 LLM。
2. `final` 或“强制切句”触发一次 LLM 推理。
3. LLM `delta` 文本按短句切块后立即送 TTS，减少感知等待。

## 4. 回合判定（Turn Detection）

采用“两段式判定”：声学端点 + 语义短等待。

### 4.1 推荐参数（中文日常对话起始值）

1. `min_speech_ms = 180`
2. `silence_for_maybe_end = 350~450ms`
3. `silence_for_force_end = 700~900ms`
4. `stability_window = 120~180ms`
5. 结尾若命中续说词（如“嗯/然后”），追加 `+250ms` 等待
6. `max_utterance_ms = 6000~8000ms`

### 4.2 最小状态机

1. `LISTENING`：持续收 `partial`。
2. `MAYBE_END`：到达短静音阈值，等待是否续说。
3. `COMMIT`：文本稳定或超时，提交 `final_text` 触发 LLM。
4. `RESPONDING`：接收 LLM 增量并播报。
5. 用户插话（有效语音 > 120ms）立即 `cancel` 并回到 `LISTENING`。

## 5. 延迟体验与优化重点

### 5.1 体验公式

用户听到机器人开口的等待时间约为：

`回合判定延迟 + LLM 首 token 延迟(TTFT) + TTS 首包延迟`

### 5.2 优化优先级

1. 优先压低 `TTFT`（最影响“聪明/灵敏”体感）。
2. 收紧回合判定窗口（但避免抢话）。
3. 降低 TTS 首包与首句合成时延。

### 5.3 工程策略

1. 长连接复用（会话常驻，避免每轮建连）。
2. 主模型走快速档，小模型快答，大模型兜底升级。
3. 控制上下文长度：滑窗 + 摘要，固定系统提示前缀。
4. 工具调用异步化：优先先给短答，再补充结果。
5. 统一埋点：`P50/P95 TTFT`、`P95 首音时间`、回合成功率。

## 6. 模型与服务选型（面向 Realtime Text）

### 6.1 可选方向

1. 云服务原生实时（WebSocket 双向）：
   1. OpenAI Realtime API
   2. Gemini Live API
   3. Qwen3-Omni-Flash-Realtime
2. 自部署实时网关：
   1. LocalAI Realtime（协议兼容层）
   2. 后挂本地或私有模型

### 6.2 选型原则

1. 优先满足双向实时协议，而非仅 `SSE stream`。
2. 用真实语料压测 200+ 轮，按 `P95 TTFT` 排序。
3. 工具调用能力与实时会话能力要同时验证。
4. 区域可用性、稳定性与成本并列考量。

## 7. 记忆与工具调用（文本旁路）

在文本主链路下，记忆和工具调用更易落地：

1. ASR `final_text` 同步写入短期记忆总线。
2. 回合前做轻量检索，拼接最小上下文。
3. 模型返回工具调用意图时，经 `Tool Gateway` 执行。
4. 工具结果回写会话，再触发补全回答。
5. 全链路保留审计事件（请求、参数、耗时、结果）。

## 8. 增加视频识别：关键帧与语义理解

### 8.1 接入方式

新增独立 `Video Pipeline`，与语音链路并行，不阻塞主对话：

1. 快速信号层（100~300ms）：检测人/物/动作变化。
2. 语义摘要层（1~2s）：关键帧理解并生成场景摘要。

### 8.2 关键帧抽取策略

1. 基础采样：`1 fps`。
2. 自适应补帧触发条件：
   1. 场景切换（SSIM/直方图差异突变）。
   2. 动作突变（光流或框位移激增）。
   3. 目标集合变化（新增/消失显著）。
3. 质量过滤：去模糊、去过暗、去低信息帧。
4. 去重：视觉 embedding 相似度阈值（如 `cosine > 0.92` 丢弃）。

### 8.3 关键帧含义理解

每帧先转结构化语义，再做时序聚合：

1. 帧级输出：`objects/actions/scene/risk`。
2. 片段级聚合：`事件开始-持续-结束`。
3. 回合注入：仅注入最近 `5~10s` 视觉摘要，控制在 `150~300 tokens`。

## 9. MVP 落地顺序

1. M1：上线 `ASR -> Realtime Text LLM -> TTS` 闭环（无视频）。
2. M2：补齐回合判定状态机 + 打断控制 + 工具调用。
3. M3：上线视频基础采样与关键帧摘要（先规则后模型）。
4. M4：统一记忆总线，打通语音/视频/工具审计与回放。

## 10. 核心风险与应对

1. 误切句导致答非所问：
   1. 通过“双阈值 + 语义延迟”减少误判。
2. TTFT 波动影响体感：
   1. 采用小模型主路由 + 限制上下文长度。
3. 视频信息噪声大：
   1. 关键帧去重 + 事件聚合，避免全量喂给 LLM。
4. 工具调用拖慢主回复：
   1. 先短答，再异步补全结果。

## 11. 评估指标（建议作为验收线）

1. `P50 TTFT < 250ms`
2. `P95 TTFT < 700ms`
3. `P95 首音时间 < 1200ms`
4. 回合成功率 `> 99%`
5. 打断生效率 `> 98%`
6. 视频事件误报率持续下降（按周评估）

---

本文件用于指导“实时文本对话主链路 + 视频语义增强”实现阶段，后续以压测数据更新阈值与选型结论。
