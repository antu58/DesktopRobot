# 与大模型实时通信 - 技术方案简述

## 1. 目标与范围

当前阶段目标：先跑通单路语音实时识别链路，形成可扩展到“连续对话 + 工具调用 + 视觉输入”的基础底座。

已覆盖能力：

1. 浏览器麦克风实时采集。
2. WebRTC 实时传输音频分片。
3. 后端接入流式 ASR（FunASR）并实时回传文字。
4. Docker 一键部署，固定端口可复现。

## 2. 技术环节

### 2.1 端侧采集与预处理

1. 网页端调用 `getUserMedia` 获取麦克风。
2. 将输入音频统一为单声道并降采样到 `16kHz`。
3. 转换为 `PCM16LE` 分片，准备实时发送。

### 2.2 实时传输与会话建立

1. 使用 WebRTC DataChannel 传输音频分片。
2. 前端先完成 ICE gathering，再提交 SDP Offer。
3. Go 后端完成 SDP Answer、ICE 状态管理与会话生命周期管理。

### 2.3 流式识别

1. Go 后端将音频分片转发给 Python ASR 侧车（WebSocket）。
2. FunASR 使用流式模型 `paraformer-zh-streaming` 按 chunk 推理。
3. ASR 结果按 partial/final 输出，持续推回 Go 服务。

### 2.4 实时文本回传

1. Go 服务将识别结果封装为 JSON 消息。
2. 通过 DataChannel 回传前端。
3. 前端逐条渲染，实现流式字幕体验。

### 2.5 部署与运行

1. 使用 Docker Compose 编排 `go-server + asr-bridge`。
2. 通过本地代理完成镜像拉取与模型下载。
3. 固定端口映射，保证团队内可重复部署。

## 3. 技术方案选型

### 3.1 传输层：WebRTC（Pion）

选型：

1. 服务端：Go + `pion/webrtc`。
2. 前端：浏览器原生 WebRTC。

原因：

1. 低延迟，适合实时语音。
2. 支持 ICE/NAT 场景，后续可扩展到音视频双流。
3. 与未来“实时对话 + 视觉输入”架构一致。

### 3.2 识别层：FunASR（流式）

选型：

1. Python 侧车 + FunASR `paraformer-zh-streaming`。

原因：

1. 中文流式识别能力成熟。
2. 可本地部署，利于后续私有化与成本控制。
3. 具备向 VAD、标点、说话人能力扩展的路径。

### 3.3 后端编排层：Go

选型：

1. Go 负责信令、会话管理、数据转发与回传。

原因：

1. 并发模型适合实时会话。
2. 二进制部署简单，容器化稳定。
3. 便于后续接入工具调用网关和会话状态机。

### 3.4 部署层：Docker Compose

选型：

1. 两容器结构：`go-server` + `asr-bridge`。

原因：

1. 环境隔离清晰，便于迭代。
2. 本地与测试环境一致性高。
3. 便于未来拆分更多能力容器（TTS、工具网关、日志服务）。

## 4. 当前固定配置

1. Web 访问端口：`18188/tcp`。
2. WebRTC ICE 端口：`19188/udp`。
3. Go 服务容器：`go-server`。
4. ASR 服务容器：`asr-bridge`。
5. 默认 ASR 模型：`paraformer-zh-streaming`。

## 5. 后续演进建议

1. 加入实时 TTS，形成“听说一体”闭环。
2. 接入 VAD 与端点检测，优化静音段处理与延迟。
3. 引入工具调用网关（Function Calling/MCP）打通对话中的工具执行。
4. 叠加视觉输入链路（摄像头帧抽样 + 视觉模型）实现多模态对话。
