# 2026-02-28 实时多模态助手完整方案

## 1. 文档目的

基于以下已完成探索，沉淀一份可直接实施的统一方案：

1. `2026-02-26-与大模型的实时通信`（实时引擎与双工作流）
2. `2026-02-27-语音长时保活与结构化回传`（Edge+Go 语音主链路已跑通）
3. `2026-02-27-视频识别服务网页测试`（本地视频快慢路径验证）
4. `2026-02-28-千问VL模型使用探索`（qwen3-vl-plus 能力与性能验证）
5. 当前新结论：采用“语音驱动 + 视觉驱动并行，语音高优先级抢占，视觉异步排队”

---

## 2. 统一结论（最终架构方向）

1. 主链路必须保持“语音优先、低延迟”：
`ASR -> LLM -> TTS`。
2. 视觉能力采用“后台持续理解 + 按需注入”：
默认不阻塞主对话，不每轮强行注入视觉。
3. 视觉相关回复分两类：
- 用户显式提问触发（如“你看这是什么”）
- 系统策略触发（如熟人出现、用户递物等高置信事件）
4. 调度规则：
- 语音可打断视觉
- 视觉不能打断语音
- 视觉结果过期即丢弃（stale discard）
5. `qwen3-vl-plus` 定位：
- 适合视觉深描与推理
- 不适合作为每轮实时主链路模型（图文时延高）

---

## 3. 现状资产与复用边界

### 3.1 已有可复用能力

1. 语音主链路（已跑通）
- 路径：`Browser Mic -> Edge(FunASR+Filter) -> Go LLM -> Edge -> Browser`
- 能力：流式回传、final聚合、pre/post token打断、长连接保活、会话短记忆

2. 视频能力（已验证原型）
- 路径：浏览器视频 -> 后端 1fps 帧处理
- 能力：YOLO/OCR/人脸特征记忆/关键事件日志
- 模式：快路径持续、慢路径关键帧触发

3. 视觉大模型能力（已验证）
- 模型：`qwen3-vl-plus`
- 接口：OpenAI兼容 `/v1/chat/completions`
- 结论：图文约 10~12s（小样本），文本约 2~3s

### 3.2 当前缺口

1. 缺少统一“多模态编排层”将语音与视觉事件合流。
2. 缺少视觉记忆总线与检索注入策略。
3. 缺少统一优先级队列与抢占规则实现。
4. 缺少“视觉驱动自动发声”的安全阈值与冷却策略。

---

## 4. 目标系统（统一架构）

```text
Browser (Mic + Camera)
  -> Edge Gateway
      - ASR stream + Turn Detection + Filler Filter
      - Frame Sampler (1fps)
      - Voice/Vision Event Router
  -> Vision Service
      - Fast CV Path (YOLO/OCR/Face/Rules)
      - Keyframe Selector (scene-change/dedup/quality)
      - Slow VLM Path (qwen3-vl-plus on demand)
      - Vision Memory Store (time-window summaries)
  -> Orchestrator (Go LLM backend evolving)
      - Priority Scheduler (voice > vision)
      - Context Assembler (text + optional vision memory)
      - OpenAI-compatible LLM client
      - Tool/Skill Gateway
  -> TTS Stream + Subtitle
```

设计原则：

1. 语音回合确定后快速响应，优先保证体感灵敏。
2. 视觉采用异步增量，不阻塞主链路。
3. 上下文注入“按需最小化”，避免视觉噪声污染对话。
4. 所有事件有 `session_id/request_id/ts_ms`，支持回放与审计。

---

## 5. 双驱动编排设计

### 5.1 语音驱动路线（高优先级）

1. Edge 产生 `voice_final` 事件。
2. 调度器立即入高优队列。
3. 若当前在跑视觉任务：
- 取消未执行视觉任务
- 标记正在执行视觉任务 `stale=true`
4. LLM 快速回复，TTS 播报。

### 5.2 视觉驱动路线（低优先级）

1. Video Service 产出结构化事件（例：`offer_object: flower`、`known_person_appeared`）。
2. 满足触发策略才进视觉队列：
- 最近 2s 无有效语音
- 置信度高于阈值
- 事件连续出现 N 帧
- 冷却窗口未命中
3. 调度器在语音空闲时执行视觉驱动回复。

### 5.3 关键优先级规则

1. `voice > vision`。
2. 视觉队列只保留“最新1~2条”同类事件（coalesce）。
3. 视觉结果如果超过新鲜度阈值（如 5~8s）则丢弃。

---

## 6. 视觉记忆机制（豆包式低延迟策略）

### 6.1 后台持续记忆，不默认注入

1. 每秒 1 帧抽样（可自适应 0.5~1fps）。
2. 快路径写入 `vision_memory`（轻量摘要）。
3. 仅在需要时触发慢路径 VLM 深描。

### 6.1.1 本地视频滑动缓存（新增约束）

1. 本地维护最近 `5~10s` 视频环形缓存，默认 `10s`。
2. 示例：采样 `10fps` 时，最大缓存 `100帧`；每新增 `10帧`，淘汰最老 `10帧`（始终保持最近窗口）。
3. 缓存元素建议包含：`ts_ms/frame/motion_score/objects/ocr`。
4. 触发视觉技能时先做 `snapshot`，避免提交过程中被新帧覆盖。
5. 语音回合中仅缓存不提交；语音结束后可提交该时间段的视觉摘要/关键帧与语音文本一并推理。

### 6.2 何时把视觉记忆注入 LLM

1. 用户显式视觉问句：
- “你看这是什么”
- “屏幕上写了什么”
- “刚才我拿的是什么”
2. 高优风险事件：
- 安全风险/异常行为/明确任务关键事件

### 6.3 两阶段应答（解决视觉5秒慢响应）

1. 第一步（<500ms）：先回执
- “我正在看这个画面，马上告诉你。”
2. 第二步（3~8s）：补最终答案
- “看到了，这是一朵花，颜色偏浅，花瓣完整。”

---

## 7. 模型与推理路由

### 7.1 本地快路径（持续运行）

1. YOLO：目标检测
2. OCR：文字提取
3. 人脸特征：熟人识别
4. 规则事件：递交动作、近距离展示、手机使用等

### 7.2 云端慢路径（按需触发）

模型：`qwen3-vl-plus`

触发条件（建议）：

1. 用户显式视觉提问
2. 快路径识别到复杂场景，需要语义推理
3. 关键帧变化显著且最近无同类深描

提交策略（避免全量送模）：

1. 不提交整个缓存窗口，优先提交关键子集。
2. 建议从 `10s` 缓存中选 `4~8帧`：
- 最近 `2s` 稠密采样（保证时效）
- 前 `8s` 稀疏关键帧（保证上下文）
3. 在满足效果前提下优先控制 `VLM_MAX_FRAMES`，降低时延与成本。

### 7.3 提示词拼装

1. 普通语音问答：仅语音文本上下文。
2. 视觉问答：
- 用户文本
- 最近视觉记忆 top-k
- 必要时加入单帧/多帧深描结果

---

## 8. 协议升级建议

在现有 `llm_request` 上新增可选字段：

```json
{
  "type": "llm_request",
  "request_id": "s-xxx-r123",
  "session_id": "s-xxx",
  "trigger_source": "voice|vision",
  "priority": "high|low",
  "text": "你看这是什么",
  "vision_context": {
    "window_ms": 8000,
    "summary": "用户将一朵花靠近摄像头并持续1.2秒",
    "objects": ["flower", "hand"],
    "ocr": [],
    "confidence": 0.91,
    "stale_after_ms": 6000
  },
  "ts_ms": 1700000000000
}
```

新增调度状态事件（返回前端）：

1. `vision_queued`
2. `vision_skipped_stale`
3. `vision_canceled_by_voice`
4. `vision_reply_sent`

---

## 9. 调度状态机（实现建议）

状态：

1. `IDLE`
2. `VOICE_RUNNING`
3. `VISION_RUNNING`
4. `INTERRUPTING_VISION`

事件：

1. `voice_final_arrived`
2. `vision_event_arrived`
3. `voice_barge_in`
4. `task_done`
5. `task_timeout`

规则：

1. `VOICE_RUNNING` 时仅缓存视觉（低优）。
2. `VISION_RUNNING` 收到语音时，立即转 `INTERRUPTING_VISION`。
3. 视觉任务被中断后结果不回写用户，只可写日志。

---

## 10. 性能目标（结合现有实测）

### 10.1 语音主链路（目标）

1. P50 首包响应（文本delta）< 800ms
2. P95 首包响应 < 1500ms
3. 打断生效率 > 98%

### 10.2 视觉链路（接受范围）

1. 快路径事件产出 < 400ms
2. 慢路径（qwen3-vl-plus）单次 4~12s 可接受
3. 显式视觉问句首回执 < 500ms
4. 最终视觉答案超时阈值建议 8~12s

---

## 11. 分阶段实施计划

### Phase A（1~2天）

1. 抽象统一事件结构（voice/vision）。
2. 在现有 Go 后端增加 `trigger_source/priority/vision_context` 兼容字段。
3. 先实现队列优先级、视觉任务 stale 丢弃、`VISION_ONLY_TRIGGER_SILENCE_MS=2000`。

### Phase B（2~4天）

1. 接入视频快路径到现有语音系统（1fps、关键事件）。
2. 建立 `vision_memory`（最近 60s 滚动窗口）。
3. 实现显式视觉问句 -> 视觉技能调用。

### Phase C（3~5天）

1. 接入 qwen3-vl-plus 慢路径深描。
2. 实现两阶段应答（先回执后补答）。
3. 加入冷却、配额、去重策略，控制成本。

### Phase D（持续优化）

1. 并发压测与阈值调优。
2. 自动评估：误报率、过期回复率、打断冲突率。
3. 引入长期记忆/MCP工具网关/纪要流融合。

---

## 12. 关键参数建议（初始值）

1. 视频采样：`1fps`
2. 视觉事件最小持续：`>=3帧`
3. 视觉触发冷却：`8~15s`
4. 视觉自动发声频率：`<=2次/分钟`
5. 视觉新鲜度：`stale_after_ms=6000`
6. 语音最小上送间隔：沿用当前 `SUBMIT_MIN_INTERVAL_MS=600`
7. 语音超时：建议 `BACKEND_REQ_TIMEOUT_S=45~60`
8. 纯视觉触发静默阈值：`VISION_ONLY_TRIGGER_SILENCE_MS=2000`
9. 视频缓存窗口：`VIDEO_CACHE_SECONDS=10`
10. 视频缓存采样：`VIDEO_CACHE_FPS=10`
11. 视觉技能单次最大帧：`VLM_MAX_FRAMES=6`

---

## 13. 风险与防护

1. 视觉误触发导致“自言自语”
- 通过冷却+置信度+连续帧确认+分钟上限抑制。
2. 视觉结果过期导致答非所问
- request_id绑定 + stale discard + 仅保留最新事件。
3. 成本飙升
- 快路径先行，慢路径按需触发，限制深描频率。
4. 上下文污染
- 默认不注入视觉；仅在显式需求时注入 top-k 摘要。

---

## 14. 验收标准（建议）

1. 连续 60 分钟会话不断链（语音主链路）。
2. 显式视觉问句命中率 > 95%。
3. 视觉回复过期率 < 2%。
4. 语音对视觉打断成功率 > 98%。
5. 单小时视觉慢路径调用次数可控（符合预算）。

---

## 15. 与现有目录的映射

1. 语音主链路代码基线：
- `/Users/zhangfeng/Desktop/Linux/DesktopRobot/项目探索内容/2026-02-27-语音长时保活与结构化回传`
2. 视频快慢路径参考实现：
- `/Users/zhangfeng/Desktop/Linux/DesktopRobot/项目探索内容/2026-02-27-视频识别服务网页测试`
3. VL模型能力与性能依据：
- `/Users/zhangfeng/Desktop/Linux/DesktopRobot/项目探索内容/2026-02-28-千问VL模型使用探索`
4. 总体实时架构基线：
- `/Users/zhangfeng/Desktop/Linux/DesktopRobot/项目探索内容/2026-02-26-与大模型的实时通信`

---

## 16. 最终建议

先把“语音高优先级 + 视觉异步记忆 + 显式问句触发视觉技能”这三件事做稳，再逐步增加视觉自动发声与复杂推理。这样能在不破坏当前语音稳定性的前提下，最快拿到豆包式体验。
