# 2026-02-27 视频识别服务方案调研（补齐现有语音链路）

## 1. 背景与目标

当前链路已跑通：

`Browser Mic -> Edge(FunASR+Filter) -> Go LLM Backend -> Edge -> Browser`

本次目标是补齐“视频服务”缺口，使系统具备：

1. 以 `1fps` 抽样关键帧并输出文本化视觉信息。
2. 与已跑通的 ASR 文本融合后送入 LLM 后端。
3. 不阻塞当前语音主链路（视频异步并行）。

---

## 2. 现状缺口（基于当前代码）

从现有实现看，视频相关能力尚未落地：

1. 浏览器侧仅采集音频（`getUserMedia({audio:..., video:false})`）。
2. Edge 仅处理 PCM + FunASR + 语音事件过滤，不处理视频帧。
3. Go 后端请求结构仅包含 `text/emotion/event/final`，没有视觉上下文字段。
4. 无关键帧抽取、去重、场景变化检测、视觉摘要服务。

结论：缺失的是一个独立 `Video Service`（视频输入 -> 关键帧筛选 -> 文本摘要 -> 向 LLM 提供结构化视觉上下文）。

---

## 3. 视频服务可选方案

## 方案A：纯本地 CV 服务（规则/检测模型为主）

路径：
`Video Ingest -> 1fps抽样 -> 关键帧筛选 -> 目标检测/OCR -> 文本模板化输出`

特点：

1. 成本可控、隐私好、可离线。
2. 对“开放语义理解”能力较弱（需要较多规则）。
3. 适合先快速稳定上线。

## 方案B：纯云端 VLM 服务（每帧/关键帧直送视觉模型）

路径：
`Video Ingest -> 1fps关键帧 -> VLM理解 -> 文本摘要`

特点：

1. 语义能力强，开发快。
2. 成本和时延随帧量增长明显。
3. 对网络与配额依赖高。

## 方案C：混合方案（推荐）

路径：
`1fps抽样 + 本地预筛(变更检测/质量过滤)` -> `仅关键帧触发VLM深描` -> `聚合文本上下文`

特点：

1. 兼顾成本、延迟、语义质量。
2. 与现有“Edge 先过滤再送 Go”理念一致。
3. 适合作为你当前架构的增量演进方案。

---

## 4. 推荐落地架构（与现有 Edge+Go 对齐）

## 4.1 新增服务边界

新增一个容器：`video-service`

```text
Browser(video/screen)
  -> Edge Frontend (video frame sender, 1fps)
  -> Video Service
      -> Keyframe Selector (scene/change/quality/dedup)
      -> Vision Analyzer (CV fast path + optional VLM deep path)
      -> Event Aggregator (5~10s window)
  -> Edge Frontend (vision text context)
  -> Go LLM Backend (fused prompt)
```

## 4.2 模块分层

1. `Ingest`：接收 JPEG/WebP 帧（建议 640p，1fps）。
2. `Sampler`：固定 1fps；必要时突变补帧（最高 2~3fps，短时）。
3. `Selector`：
   - 场景变化检测（直方图差异 / scene score）。
   - 质量过滤（模糊、过暗、纯色低信息）。
   - 去重（embedding 或感知哈希）。
4. `Analyzer`：
   - 快路径：人/物/动作/屏幕文字 OCR。
   - 深路径：仅对“候选关键帧”调用 VLM 生成语义描述。
5. `Aggregator`：输出最近 `5~10s` 的视觉摘要，限制 `150~300 tokens`。

---

## 5. 与现有协议的对接方式

## 5.1 不改 Go 协议的最小落地（MVP）

Edge 在提交语音文本时，拼接视觉上下文到 `text` 字段：

```text
{ASR final text}

[vision_context]
- t-2s: 用户打开IDE，屏幕显示Go编译报错
- t-1s: 检测到终端命令 go test ./...
```

优点：Go 后端零改动即可验证效果。

## 5.2 推荐的协议演进（M2）

在 `llm_request` 增加可选字段：

```json
{
  "type": "llm_request",
  "request_id": "s-xxx-r12",
  "session_id": "s-xxx",
  "text": "帮我看下这个报错",
  "emotion": "EMO_UNKNOWN",
  "event": "Speech",
  "final": true,
  "ts_ms": 1700000000000,
  "vision_context": {
    "window_ms": 8000,
    "summary": "用户在终端执行 go test，出现 connection refused。",
    "objects": ["terminal", "IDE"],
    "ocr": ["go test ./...", "connection refused"],
    "risk_flags": []
  }
}
```

---

## 6. 关键工程参数（建议起始值）

1. 抽帧率：`1fps`（固定主采样）。
2. 分辨率：`960x540` 或 `1280x720`（按设备性能切换）。
3. 场景突变阈值：先用 `scene score > 0.4`（后续按数据调参）。
4. 去重阈值：`cosine > 0.92` 或 `pHash distance <= 6` 丢弃。
5. 聚合窗口：`8s`，每次注入最多 `220 tokens`。
6. 视觉上送节流：`>= 1500ms` 一次（除非触发风险事件）。

---

## 7. 分阶段实施建议

## M1（1~2 天）：可跑通视频文本化最小链路

1. Browser 打开 `video/screen` 捕获（仅 1fps）。
2. 新增 `video-service`：接收帧，做简单变化检测 + OCR，回文本摘要。
3. Edge 将视觉摘要拼入 `text` 后送 Go。

## M2（2~4 天）：增强关键帧质量

1. 增加场景突变 + 去重 + 质量过滤。
2. 增加视觉事件类型（`scene_change/object_change/risk`）。
3. 将 `vision_context` 协议字段化（不再依赖拼文本）。

## M3（4~7 天）：混合语义增强

1. 仅对“候选关键帧”调用 VLM 深描。
2. 做视觉事件时序聚合（开始-持续-结束）。
3. 建立离线回放与误报/漏报评测集。

---

## 8. 结论（本轮建议）

本项目缺失的不是“某个模型”，而是一层独立的视频服务编排能力。  
建议采用**混合方案（本地预筛 + 关键帧触发VLM）**，先走 M1 最小可用路径，保持对当前语音链路和 Go 后端最小侵入。

---

## 参考资料（本轮调研）

1. FFmpeg `fps` 过滤器（固定帧率抽样）：<https://ffmpeg.org/ffmpeg-filters.html>
2. FFmpeg `select` 的 `scene` 指标与示例阈值（0.3~0.5）：<https://ffmpeg.org/ffmpeg-filters.html>
3. FFmpeg `scdet` 场景变化检测（含阈值说明）：<https://ffmpeg.org/ffmpeg-filters.html>
4. PySceneDetect 检测算法（Content/Adaptive/Threshold）：<https://www.scenedetect.com/docs/api/detectors.html>
5. OpenAI Images & Vision（`input_image`、`detail`、图像输入限制与计费规则）：<https://platform.openai.com/docs/guides/images-vision>
6. Ultralytics YOLO 流式推理与多路流输入：<https://docs.ultralytics.com/modes/predict/>
7. Ultralytics YOLO 多目标跟踪（BoT-SORT/ByteTrack）：<https://docs.ultralytics.com/modes/track/>
8. MDN `getDisplayMedia`（浏览器屏幕采集）与 `getUserMedia`：<https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getDisplayMedia> / <https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia>
