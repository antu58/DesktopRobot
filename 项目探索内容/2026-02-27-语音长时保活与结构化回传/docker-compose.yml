services:
  go-llm:
    build:
      context: ./go-llm-backend
      dockerfile: Dockerfile
    restart: unless-stopped
    env_file:
      - ../../Soul/.env
    environment:
      PORT: ${BACKEND_PORT_INNER:-8090}
      LLM_TIMEOUT_S: ${LLM_TIMEOUT_S:-90}
      CHAT_HISTORY_LIMIT: ${CHAT_HISTORY_LIMIT:-20}
      LLM_SYSTEM_PROMPT: ${LLM_SYSTEM_PROMPT:-你是语音助手，请基于用户输入直接给出简洁有帮助的中文回答。}
    ports:
      - "127.0.0.1:${BACKEND_PORT:-18090}:8090"

  edge-frontend:
    build:
      context: ./edge-frontend
      dockerfile: Dockerfile
      args:
        HTTP_PROXY: ${BUILD_HTTP_PROXY:-}
        HTTPS_PROXY: ${BUILD_HTTPS_PROXY:-}
        ALL_PROXY: ${BUILD_ALL_PROXY:-}
    restart: unless-stopped
    depends_on:
      - go-llm
    env_file:
      - ../../Soul/.env
    environment:
      BACKEND_WS_URL: ${BACKEND_WS_URL:-ws://go-llm:8090/ws/edge}
      BACKEND_REQ_TIMEOUT_S: ${BACKEND_REQ_TIMEOUT_S:-30}
      BACKEND_CONN_TIMEOUT_S: ${BACKEND_CONN_TIMEOUT_S:-8}
      BACKEND_RECONNECT_S: ${BACKEND_RECONNECT_S:-1.5}
      BACKEND_MAX_PENDING: ${BACKEND_MAX_PENDING:-8}
      BACKEND_WS_PING_INTERVAL_S: ${BACKEND_WS_PING_INTERVAL_S:-20}
      BACKEND_WS_PING_TIMEOUT_S: ${BACKEND_WS_PING_TIMEOUT_S:-0}
      STRICT_MODEL: ${STRICT_MODEL:-1}
      FUNASR_MODEL: ${FUNASR_MODEL:-iic/SenseVoiceSmall}
      VAD_MODEL: ${VAD_MODEL:-fsmn-vad}
      FUNASR_HUB: ${FUNASR_HUB:-ms}
      FUNASR_DEVICE: ${FUNASR_DEVICE:-cpu}
      VAD_CHUNK_MS: ${VAD_CHUNK_MS:-200}
      MAX_SEGMENT_MS: ${MAX_SEGMENT_MS:-30000}
      PRE_ROLL_MS: ${PRE_ROLL_MS:-120}
      ASR_LANGUAGE: ${ASR_LANGUAGE:-auto}
      ASR_USE_ITN: ${ASR_USE_ITN:-1}
      ASR_BATCH_SIZE_S: ${ASR_BATCH_SIZE_S:-60}
      SUBMIT_MIN_TEXT_CHARS: ${SUBMIT_MIN_TEXT_CHARS:-2}
      SUBMIT_REQUIRE_SPEECH: ${SUBMIT_REQUIRE_SPEECH:-1}
      SUBMIT_MIN_INTERVAL_MS: ${SUBMIT_MIN_INTERVAL_MS:-600}
      FILTER_FILLER: ${FILTER_FILLER:-1}
      FILLER_MAX_CHARS: ${FILLER_MAX_CHARS:-8}
      FINAL_MERGE_GAP_MS: ${FINAL_MERGE_GAP_MS:-500}
      FINAL_MERGE_MAX_MS: ${FINAL_MERGE_MAX_MS:-2200}
      INTERRUPT_PRE_TOKEN: ${INTERRUPT_PRE_TOKEN:-1}
      INTERRUPT_POST_TOKEN_MODE: ${INTERRUPT_POST_TOKEN_MODE:-conditional}
      INTERRUPT_MIN_CHARS: ${INTERRUPT_MIN_CHARS:-6}
      MODELSCOPE_CACHE: /models/modelscope
      HF_HOME: /models/huggingface
      HTTP_PROXY: ${RUNTIME_HTTP_PROXY:-}
      HTTPS_PROXY: ${RUNTIME_HTTPS_PROXY:-}
      ALL_PROXY: ${RUNTIME_ALL_PROXY:-}
      NO_PROXY: ${RUNTIME_NO_PROXY:-127.0.0.1,localhost,edge-frontend,go-llm}
    volumes:
      - ./models:/models
    ports:
      - "127.0.0.1:${WEB_PORT:-18288}:8080"
